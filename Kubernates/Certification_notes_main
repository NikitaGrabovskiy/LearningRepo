        ***
Worker Node

What Runs on a Worker Node:
Pods
Deployments, DaemonSets, StatefulSets ‚Äî all land on worker nodes
Node-level add-ons (like monitoring agents)

        ***
Master Node has control plain components :

    1) Etcd

    Etcd is a distributed, consistent key-value store used by Kubernetes as its primary database.
    It stores all the cluster‚Äôs state and metadata‚Äîsuch as nodes, Pods, ConfigMaps, Secrets, networking configurations, and more.
    Because etcd is highly available, fault-tolerant, and uses the Raft consensus algorithm,
    Kubernetes can reliably coordinate the entire cluster even if some nodes fail.
    In simple terms:
    üëâ If etcd is down, the Kubernetes API has no source of truth and the cluster cannot function properly.

    2) Kube schedulers:
        * Node controller
        * Replication controller

    3) Kube-apiserver

    The kube-apiserver is the central entry point to the Kubernetes control plane.
    It exposes the Kubernetes API and is responsible for receiving all requests‚Äîwhether from kubectl,
    controllers, schedulers, or other components‚Äîand validating and processing them.
    It reads and writes cluster state to etcd, enforces authentication/authorization,
    and acts as the communication hub for all control-plane components and worker nodes.

    In simple terms:
    üëâ It is the ‚Äúfront door‚Äù of the Kubernetes cluster and the only component that talks directly to etcd.

      ***

 Docker or other container technology needs to be installed on all nodes, including Master Node

      ***
  Kubelet is an agent that runs on each node on a cluster (Like a capitan on a ship in the example)
  It listens instractions from kube-apiserver and create or destroy pods on the node

    ***
Kube proxy also installed on each node. It allows communication between worker nodes

    ***

Container Runtime Interface (CRI) - allows Kubernates to use different containers (Not just docker)

    ***

You do not need to install docker. It is possible to run containers just with Container D

    ***

You can install etcd cluster, run put and get commands

Only when the change (pods, deployments etc) updated in etcd - it is considered complete

in the etcd.service you can find:
    --advertise-client-urls http://${INTERNAL_IP}:2379         (it is the url that can be used to access the etcd)

If you used kubeadm to install kubernates - it will install the etcd as a pod in kubeadm-system

If you have several etcd-servers running for high availability - you need to add a -- config option to
etcd.service with info of other servers so the servers would know about each other

    ***

    Kube-api server

When you run kubectl command - kube-apiserver called

If you run get pods command the kube-apiserver access etcd cluster to get the info and send it back to the user

If you run create pod command - kube-apiserver will update etcd and return a success to user. After kube-scheduler will see that
there is an unassigned pod and will create it in the node. The scheduler will call kube-apiserver again with request to schedule
a pod of one of the nodes. The kube-apiservier will send a command to the node kubelet to schedule the pod. Once done
the kubelet of the node will send a updated status to kube-apiserver and it will update the status in etcd.

use kube-apiserver.service to add configuration parameters to kube-apiserver

If you use kubeadm tool to install kubernates it will install it as a separate node:

kube-apiserver-master


                               *** Controller-Manager ***

Controller is a process that continuously monitors the state of various components within the system and works torwards
brinning system to the desired state.

For example :
    Node controller constantly monitoring states of all the nodes in the cluster and takes the necessary actions to keep
    the applications running. It does that througth kube-apiserver.
    It constantly gets status of the nodes. If if nodes stops to replying to the calls - contoroller marks it as unreachable.
    It waits for 5 minutes to node get back to it's healthy state. If it not - it marks it as Note ready and assign it's
    pods to another nodes


    Next controller - Replication controller - makes sure the desired number of replicas created

There is an option to select which controllers do you want to enable. But default they all be enabled.
If any of your contontrollers do not work or exist - that is the first thing to check.


                         *** Kube-scheduler ***

Kube-scheduler responsible for desiding which pod goes to which node.
Kubelet of the node itself - is actually create the pods

                              *** Kubelet ***

 It receives an instructions from kube-apiserver. When it asks available container runtime (Container D) to
 pull the image and create pods

    If you use Kubeadm tool to install cluster - it does not automatically install kubelet on the nodes !!!
    That is an important difference from other components!!!
    You must always manually install kubelet on your worker nodes !!!

    1) Download installer
    2) Install it
    3) Run it as a service


    *** Kube proxy ***
It is a process that runs on each node on a kubernates cluster. It's job is to search for a new services,
and every time a new service created, it creates a rules to forward traffic from nodes to the new services.

Kubeadm tool deploys kube-proxy to each node of the cluster


                     *** Yaml in Kubernates ***

Defenition yaml files always contain the following 4 fields (Root level properties) :

apiVersion:
kind:
metadata:


spec:


    Examples

metadata:
    name: myapp-pod
    labels:
        app: myapp
        type: front-end


                             *** Replica Set ***

ReplicationController is a older object (Similar functionality to ReplicaSet)

ReplicatSet has an extra spec - selector (Which ReplicationController does not have as required)

Example:

    selector:
        matchLabels:
            type: front-end

 The selector allows to create pods using containers which are not part of template (where you specify containers inside
 replica set definition file itself). Helpful if for example - there are some pods were created before
 replica set

The replica set will also manage the existed, already created pods with the same label.

ReplicaSet can only use template section to create a new pods

                            *** Deployments ***

Rolling update is then you update pods to newer version one after another

Deployments is higher in hierarchy that replica set. It allows to update underlieng instances  seamlessle using rolling updates,
undo changes and pause and resume changes as required.

Yaml for deployment looks like the replica set yaml but the kind is Deployment

Deployment automatically creates a replica set so if you run a
    kubectl get replicaset - you will see a new one created in the name of the deployment

                            *** NOTE ***

In yaml file Kind always case sensitive

Kind: Deployment

                            *** Service ***

A Service in Kubernetes provides a stable IP address, DNS name, and load-balancing layer
for accessing a group of Pods. Since Pods are dynamic and their IPs change when they restart
or move between nodes, a Service ensures clients always have a consistent way to reach them.
It uses label selectors to find matching Pods and distributes traffic across them through
kube-proxy, making internal or external communication reliable regardless of Pod churn.



                Service type  NodePort - maps pod to a node port

apiVersion: v1
kind: Service
metadata:
  name: myapp-nodeport
spec:
  type: NodePort            # Exposes the service externally on each node‚Äôs IP at a static port
  selector:                 # Defines which Pods this service sends traffic to
    app: myapp              # Must match the Pod label "app=myapp"
  ports:
    - port: 80              # The Service port (clients inside cluster use this)
      targetPort: 8080      # The port on the Pod container
      nodePort: 30080       # External static port on the Node (range 30000‚Äì32767)
      protocol: TCP         # Protocol used for the traffic (default is TCP)


port is only required field - if target port is not specified - it will assume that is is the same as port
If nodePort is not specified - it will assume that it is the same as port

If pods deployed on different nodes - it will map each pod to its node on the same port. There will be a single
service created.


                        A ClusterIP is the default Service type in Kubernetes and is used to
                        expose your application inside the cluster only.

Key points
* It gives the Service a stable internal IP address (ClusterIP).
* The Service is not accessible from outside the cluster.
* Used for internal communication between microservices (e.g., frontend ‚Üí backend, API ‚Üí DB proxies).
* Kubernetes load-balances traffic across the Pods that match its selector.
* Each Service also gets an internal DNS name like:
    myservice.default.svc.cluster.local.

ClusterIP = internal-only load balancer with a stable IP for Pods.

                    LoadBalancer type - only supported in cloud (AWS, GCP, Azure), if you run it outside of
supported cloud it will behave as nodePort type


                *** Namespaces ***


            kube-system namespace

This namespace contains core Kubernetes system components that the cluster needs to function.
Examples running in kube-system:

* kube-apiserver
* kube-controller-manager
* kube-scheduler
* kube-dns / coredns
* kube-proxy
* CNI plugin pods (Calico, Flannel, Weave, etc.)

You should not deploy your own applications into this namespace.

In short:
kube-system = Kubernetes‚Äô internal control-plane and system pods.

        kube-public namespace

A readable-by-everyone namespace (even unauthenticated users).
Usually nearly empty.
Kubernetes uses it mainly for sharing public information across the cluster.
Often contains a ConfigMap called cluster-info, which is used by kubeadm during cluster bootstrap.


You can assign a quota (limit) of resources consumed by namespace - and kubernates will ensure that resources
in the namespace does not consume more

You can add namespace option to medadata section of pod definition file - it will ensure that pod will always
be created in the namespace

                            Resource quota yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-namespace-quota
  namespace: my-namespace
spec:
  hard:
    pods: "10"                 # Max number of Pods allowed
    requests.cpu: "2"          # Total CPU requests cannot exceed 2 cores
    requests.memory: "4Gi"     # Total memory requests cannot exceed 4Gi
    limits.cpu: "4"            # Total CPU limits cannot exceed 4 cores
    limits.memory: "8Gi"       # Total memory limits cannot exceed 8Gi
    services: "5"              # Max number of Services allowed
    configmaps: "10"           # Max number of ConfigMaps
    persistentvolumeclaims: "4"  # Max PVCs in this namespace


                                            Scheduling section

There is a "nodeName" specification that kubernates attach to pod yaml after pod is schedulied (You do not
specify the property yourself) - Scheduler looking for a nodes that does not have the property and tries to schedule it.

If there is not she

To manually schedule a pod - you need to manually add a node to yaml file. But you can only do it at pod creation time.
If you do it after pod is created - kubernates will not allow you to do that.

If you want to do it after pod is created - you need to create a Binding object and make a post call api. (It will mimic
what the actual pod does.)

        Object yaml:

apiVersion: v1 kind: Pod metadata:
name: nginx labels:
name: nginx spec:
containers:
- name: nginx
image: nginx ports:
- containerPort: 8080 nodeName: node02

        The api call to schedule pod using yaml :

 curl --header "Content-Type:application/json" --request
 POST --data http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/


                                Labels

Note: when you create a relica set or service - you can spesify labels in 2 places :
    One for discovery of the replica set and one inside template for discovery of pods.

                            Difference between labels and annotations:


Labels ‚Üí used to identify and select resources (Services, Deployments use them)
Annotations ‚Üí used to store extra information (for tools, humans, metadata)

Key difference:

Labels are queryable & affect behavior
Annotations are not queryable & do not affect behavior

Rule of thumb:
Labels = selection
Annotations = information

                    Taints and Tolerations

Taints ‚Üí applied to nodes to repel Pods
Tolerations ‚Üí applied to Pods to allow them onto tainted nodes

üëâ Pods run on a tainted node only if they have a matching toleration.


To add tolerations to pod:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
  containers:
  - name: app
    image: nginx