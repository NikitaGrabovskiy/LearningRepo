        ***
Worker Node

What Runs on a Worker Node:
Pods
Deployments, DaemonSets, StatefulSets ‚Äî all land on worker nodes
Node-level add-ons (like monitoring agents)

        ***
Master Node has control plain components :

    1) Etcd

    Etcd is a distributed, consistent key-value store used by Kubernetes as its primary database.
    It stores all the cluster‚Äôs state and metadata‚Äîsuch as nodes, Pods, ConfigMaps, Secrets, networking configurations, and more.
    Because etcd is highly available, fault-tolerant, and uses the Raft consensus algorithm,
    Kubernetes can reliably coordinate the entire cluster even if some nodes fail.
    In simple terms:
    üëâ If etcd is down, the Kubernetes API has no source of truth and the cluster cannot function properly.

    2) Kube schedulers:
        * Node controller
        * Replication controller

    3) Kube-apiserver

    The kube-apiserver is the central entry point to the Kubernetes control plane.
    It exposes the Kubernetes API and is responsible for receiving all requests‚Äîwhether from kubectl,
    controllers, schedulers, or other components‚Äîand validating and processing them.
    It reads and writes cluster state to etcd, enforces authentication/authorization,
    and acts as the communication hub for all control-plane components and worker nodes.

    In simple terms:
    üëâ It is the ‚Äúfront door‚Äù of the Kubernetes cluster and the only component that talks directly to etcd.

      ***

 Docker or other container technology needs to be installed on all nodes, including Master Node

      ***
  Kubelet is an agent that runs on each node on a cluster (Like a capitan on a ship in the example)
  It listens instractions from kube-apiserver and create or destroy pods on the node

    ***
Kube proxy also installed on each node. It allows communication between worker nodes

    ***

Container Runtime Interface (CRI) - allows Kubernates to use different containers (Not just docker)

    ***

You do not need to install docker. It is possible to run containers just with Container D

    ***

You can install etcd cluster, run put and get commands

Only when the change (pods, deployments etc) updated in etcd - it is considered complete

in the etcd.service you can find:
    --advertise-client-urls http://${INTERNAL_IP}:2379         (it is the url that can be used to access the etcd)

If you used kubeadm to install kubernates - it will install the etcd as a pod in kubeadm-system

If you have several etcd-servers running for high availability - you need to add a -- config option to
etcd.service with info of other servers so the servers would know about each other

    ***

    Kube-api server

When you run kubectl command - kube-apiserver called

If you run get pods command the kube-apiserver access etcd cluster to get the info and send it back to the user

If you run create pod command - kube-apiserver will update etcd and return a success to user. After kube-scheduler will see that
there is an unassigned pod and will create it in the node. The scheduler will call kube-apiserver again with request to schedule
a pod of one of the nodes. The kube-apiservier will send a command to the node kubelet to schedule the pod. Once done
the kubelet of the node will send a updated status to kube-apiserver and it will update the status in etcd.

use kube-apiserver.service to add configuration parameters to kube-apiserver

If you use kubeadm tool to install kubernates it will install it as a separate node:

kube-apiserver-master


                               *** Controller-Manager ***

Controller is a process that continuously monitors the state of various components within the system and works torwards
brinning system to the desired state.

For example :
    Node controller constantly monitoring states of all the nodes in the cluster and takes the necessary actions to keep
    the applications running. It does that througth kube-apiserver.
    It constantly gets status of the nodes. If if nodes stops to replying to the calls - contoroller marks it as unreachable.
    It waits for 5 minutes to node get back to it's healthy state. If it not - it marks it as Note ready and assign it's
    pods to another nodes


    Next controller - Replication controller - makes sure the desired number of replicas created

There is an option to select which controllers do you want to enable. But default they all be enabled.
If any of your contontrollers do not work or exist - that is the first thing to check.


                         *** Kube-scheduler ***

Kube-scheduler responsible for desiding which pod goes to which node.
Kubelet of the node itself - is actually create the pods

                              *** Kubelet ***

 It receives an instructions from kube-apiserver. When it asks available container runtime (Container D) to
 pull the image and create pods

    If you use Kubeadm tool to install cluster - it does not automatically install kubelet on the nodes !!!
    That is an important difference from other components!!!
    You must always manually install kubelet on your worker nodes !!!

    1) Download installer
    2) Install it
    3) Run it as a service


    *** Kube proxy ***
It is a process that runs on each node on a kubernates cluster. It's job is to search for a new services,
and every time a new service created, it creates a rules to forward traffic from nodes to the new services.

Kubeadm tool deploys kube-proxy to each node of the cluster


                     *** Yaml in Kubernates ***

Defenition yaml files always contain the following 4 fields (Root level properties) :

apiVersion:
kind:
metadata:


spec:


    Examples

metadata:
    name: myapp-pod
    labels:
        app: myapp
        type: front-end


                             *** Replica Set ***

ReplicationController is a older object (Similar functionality to ReplicaSet)

ReplicatSet has an extra spec - selector (Which ReplicationController does not have as required)

Example:

    selector:
        matchLabels:
            type: front-end

 The selector allows to create pods using containers which are not part of template (where you specify containers inside
 replica set definition file itself). Helpful if for example - there are some pods were created before
 replica set

The replica set will also manage the existed, already created pods with the same label.

ReplicaSet can only use template section to create a new pods

                            *** Deployments ***

Rolling update is then you update pods to newer version one after another

Deployments is higher in hierarchy that replica set. It allows to update underlieng instances  seamlessle using rolling updates,
undo changes and pause and resume changes as required.

Yaml for deployment looks like the replica set yaml but the kind is Deployment

Deployment automatically creates a replica set so if you run a
    kubectl get replicaset - you will see a new one created in the name of the deployment

                            *** NOTE ***

In yaml file Kind always case sensitive

Kind: Deployment

                            *** Service ***

A Service in Kubernetes provides a stable IP address, DNS name, and load-balancing layer
for accessing a group of Pods. Since Pods are dynamic and their IPs change when they restart
or move between nodes, a Service ensures clients always have a consistent way to reach them.
It uses label selectors to find matching Pods and distributes traffic across them through
kube-proxy, making internal or external communication reliable regardless of Pod churn.



                Service type  NodePort - maps pod to a node port

apiVersion: v1
kind: Service
metadata:
  name: myapp-nodeport
spec:
  type: NodePort            # Exposes the service externally on each node‚Äôs IP at a static port
  selector:                 # Defines which Pods this service sends traffic to
    app: myapp              # Must match the Pod label "app=myapp"
  ports:
    - port: 80              # The Service port (clients inside cluster use this)
      targetPort: 8080      # The port on the Pod container
      nodePort: 30080       # External static port on the Node (range 30000‚Äì32767)
      protocol: TCP         # Protocol used for the traffic (default is TCP)


port is only required field - if target port is not specified - it will assume that is is the same as port
If nodePort is not specified - it will assume that it is the same as port

If pods deployed on different nodes - it will map each pod to its node on the same port. There will be a single
service created.


                        A ClusterIP is the default Service type in Kubernetes and is used to
                        expose your application inside the cluster only.

Key points
* It gives the Service a stable internal IP address (ClusterIP).
* The Service is not accessible from outside the cluster.
* Used for internal communication between microservices (e.g., frontend ‚Üí backend, API ‚Üí DB proxies).
* Kubernetes load-balances traffic across the Pods that match its selector.
* Each Service also gets an internal DNS name like:
    myservice.default.svc.cluster.local.

ClusterIP = internal-only load balancer with a stable IP for Pods.

                    LoadBalancer type - only supported in cloud (AWS, GCP, Azure), if you run it outside of
supported cloud it will behave as nodePort type


                *** Namespaces ***


            kube-system namespace

This namespace contains core Kubernetes system components that the cluster needs to function.
Examples running in kube-system:

* kube-apiserver
* kube-controller-manager
* kube-scheduler
* kube-dns / coredns
* kube-proxy
* CNI plugin pods (Calico, Flannel, Weave, etc.)

You should not deploy your own applications into this namespace.

In short:
kube-system = Kubernetes‚Äô internal control-plane and system pods.

        kube-public namespace

A readable-by-everyone namespace (even unauthenticated users).
Usually nearly empty.
Kubernetes uses it mainly for sharing public information across the cluster.
Often contains a ConfigMap called cluster-info, which is used by kubeadm during cluster bootstrap.


You can assign a quota (limit) of resources consumed by namespace - and kubernates will ensure that resources
in the namespace does not consume more

You can add namespace option to medadata section of pod definition file - it will ensure that pod will always
be created in the namespace

                            Resource quota yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-namespace-quota
  namespace: my-namespace
spec:
  hard:
    pods: "10"                 # Max number of Pods allowed
    requests.cpu: "2"          # Total CPU requests cannot exceed 2 cores
    requests.memory: "4Gi"     # Total memory requests cannot exceed 4Gi
    limits.cpu: "4"            # Total CPU limits cannot exceed 4 cores
    limits.memory: "8Gi"       # Total memory limits cannot exceed 8Gi
    services: "5"              # Max number of Services allowed
    configmaps: "10"           # Max number of ConfigMaps
    persistentvolumeclaims: "4"  # Max PVCs in this namespace


                                            Scheduling section

There is a "nodeName" specification that kubernates attach to pod yaml after pod is schedulied (You do not
specify the property yourself) - Scheduler looking for a nodes that does not have the property and tries to schedule it.

If there is not she

To manually schedule a pod - you need to manually add a node to yaml file. But you can only do it at pod creation time.
If you do it after pod is created - kubernates will not allow you to do that.

If you want to do it after pod is created - you need to create a Binding object and make a post call api. (It will mimic
what the actual pod does.)

        Object yaml:

apiVersion: v1 kind: Pod metadata:
name: nginx labels:
name: nginx spec:
containers:
- name: nginx
image: nginx ports:
- containerPort: 8080 nodeName: node02

        The api call to schedule pod using yaml :

 curl --header "Content-Type:application/json" --request
 POST --data http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/


                                Labels

Note: when you create a relica set or service - you can spesify labels in 2 places :
    One for discovery of the replica set and one inside template for discovery of pods.

                            Difference between labels and annotations:


Labels ‚Üí used to identify and select resources (Services, Deployments use them)
Annotations ‚Üí used to store extra information (for tools, humans, metadata)

Key difference:

Labels are queryable & affect behavior
Annotations are not queryable & do not affect behavior

Rule of thumb:
Labels = selection
Annotations = information

                    Taints and Tolerations

Taints ‚Üí applied to nodes to repel Pods
Tolerations ‚Üí applied to Pods to allow them onto tainted nodes

üëâ Pods run on a tainted node only if they have a matching toleration.


To add tolerations to pod:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
  containers:
  - name: app
    image: nginx


                            Node selectors

You can add nodeSelector to a pod - to specify that the pod needs to be placed on nodes with specific labels only
You need to add:

    nodeSelector:
        size: Large

And add label to the specific nodes:

    size: Large

                            Node Affinity
It used to create complex expressions for selecting node for pod (Node selectors can only specify one node label)

Example:

apiVersion: v1
kind: Pod
metadata:
  name: affinity-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - us-east-1a
  containers:
  - name: app
    image: nginx

                    Types of nodeAffinity:


requiredDuringSchedulingIgnoredDuringExecution

        Hard rule
        Pod will not be scheduled if not matched

preferredDuringSchedulingIgnoredDuringExecution

        Soft rule
        Scheduler tries to match, but may still schedule elsewhere


                            *** Resource Requests ***

You can specify minimum resource requirements for the pod and limit - how much resources can it use:

apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "256Mi"

If pod constatnly consumes more memory when specified - it will be killed with OOM error (Out of memory)

                            LimitRange

* Works on namespace level, only within namespace
* Enforces min / max CPU & memory per container
* Applies default requests & limits if Pod doesn‚Äôt specify them
* Validates Pods at creation time
üëâ If a Pod exceeds max or is below min ‚Üí rejected.

LimitRange does NOT affect existing Pods.

Why?
LimitRange is enforced only at Pod creation time
Kubernetes does not mutate running Pods


apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
  namespace: default
spec:
  limits:
  - type: Container
    min:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "1"
      memory: "1Gi"
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "200m"
      memory: "256Mi"


                            ResourceQuota

ResourceQuota limits the total amount of resources a namespace can use.
It prevents one team or app from consuming everything in the cluster.

Applies at namespace level
Blocks creation when limits are exceeded
Counts sum of all Pods in the namespace
üëâ If total usage exceeds quota ‚Üí new resources are rejected

        Example :

apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "2"
    requests.memory: 4Gi
    limits.cpu: "4"
    limits.memory: 8Gi


                                            DaemonSet

A DaemonSet ensures that one Pod runs on every (or selected) node in the cluster.

What it‚Äôs used for
   * Node-level agents that must run everywhere, like:
   * log collectors
   * monitoring agents
   * networking components

Key behavior
    * New node added ‚Üí Pod is created automatically
    * Node removed ‚Üí Pod is removed
    * Usually 1 Pod per node

  *** Static pods ***

You can create pods (Only pods, not deployments or relicaSets or other resources) by placing a pod definition yaml
 in that location on node:

/etc/kubernetes/manifests

(Remember defautl link)

The location can be different and it passed to kubelet.service when it started, this is the condif:

--pod-manifest-path=/etc/kubernetes/manifests

Another way to specify the path is to specify another config file:

--config=kubeconfig.yaml

Iside the kubeconfig.yaml put the line:

staticPodPath: /etc/kubernetes/manifests


The Kubelet (Capitan of the ship) will create the pods from the yaml and it will make sure it they alive,
it will restart it if ineeded

When the pods created you can see them by running (rest of Kubernates cluster might not get available, but the pods will still be created):
    docker ps

You can see the pod with kubectl (if the rest of the cluster is available) but you cannot delete of modify static
pods using kubectl. You can noly modify them by changeing yaml file.

Note:
    To check if the pod is static - get pod as yaml (-0 yaml) and
    Check owner section.
    It should be :


    OwnerReferences:
        kind: Node
        name: controlplane

You can check kubelet config under :
/var/lib/kubelet



                            Priority classes

They do not belong to a specific namespace - they avalable on the whole cluster,
 can be used by any pod on any namespace

Range of numbers for priority from minus 2 Bulling and handred something to 1 billion

to list priority classes:

   kubectl get priorityclass


        Example:

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "High priority workload"


        Pod using it:

apiVersion: v1
kind: Pod
metadata:
  name: critical-pod
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: nginx



By default each pod has a priority value of zero.  If you want change it - you create  a new PriorityClass and add a
property :

    globalDefault: true

Another field you can add to PriorityClass is :

    preeemptionPolicy: PreemptLowerPriority

It will decide if the existed lower priority pods needs to be deleted or not if no capacity left

    You can seet it to "never" - it will never delete existing pods


                            Multiple Schedulers:

You can add:

SchedulerName: <SCHEDULER_NAME>

to pod spec - so it will be picked up by a specific scheduler

                            Admission Controllers

Kubernetes Admission Controllers are request filters that run after authentication & authorization but before an object (Pod, Deployment, etc.) is stored in etcd.
They can allow, reject, or modify requests.

In one line

‚û°Ô∏è They enforce cluster policies and apply defaults at creation/update time.

        Example:

NamespaceLifecycle Admission Controller

What it does:
* Checks that a namespace exists
* Rejects requests to create resources in a non-existent namespace
* Prevents creating new resources in a namespace that is being deleted (Terminating)


                    APPLICATION LIFECYCLE MANAGEMENT

Deployment strategies :

             1) Recreate - when old deleted and new created after (Application downtime during deployment)

             2) Rolling update - when you delete old pods and create a new version one by one - THIS IS DEFAULT (No downtime)


                        If you need to update deployment image :

kubectl set image <DEPLOYMENT NAME> <IMAGE NAME>

        Note: it will not update the definition file

                    To rollback to a previous version of deployment:

kubectl rollout undo deployment <DEPLOYMENT NAME>


                                                    ***

apiVersion: v1
kind: Pod
metadata:
  name: echo-pod
spec:
  containers:
    - name: echo-container
      image: ubuntu
      command: ["echo"]
      args: ["Hello from Kubernetes"]


command
The executable that starts the container.
üëâ It overrides Docker ENTRYPOINT.

args
The parameters passed to that executable.
üëâ It overrides Docker CMD.

Example:
    command: ["ls"]
    args: ["-l", "/"]

                                            *** How to declare environment variables in container ***

                                             Use env :

apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "echo $APP_NAME && echo $APP_ENV && sleep 3600"]
    env:
    - name: APP_NAME
      value: "payment-service"
    - name: APP_ENV
      value: "dev"

                                        *** configMap ***

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_NAME: payment-service
  APP_ENV: dev


                    Pod that uses the map :

apiVersion: v1
kind: Pod
metadata:
  name: configmap-env-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo APP_NAME=$APP_NAME && echo APP_ENV=$APP_ENV && sleep 3600"]
    env:
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_NAME
    - name: APP_ENV
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_ENV


                                    ***

                                Secrets

                Create from literal

kubectl create secret generic db-secret \
  --from-literal=username=dbuser \
  --from-literal=password=supersecret123

                create from properties

kubectl create secret generic api-secret \
  --from-file=api-key.txt

                Use in Pod by specifying specific keys

apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
    - name: app
      image: nginx
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password


              Use in pod without specifying a specific key

apiVersion: v1
kind: Pod
metadata:
  name: app-with-secret
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "env && sleep 3600"]
    envFrom:
    - secretRef:
        name: db-secret


            Note : secretKeyRef use env vs secretRef use envFrom !!!

                            *** Multy container ***

There are 3 design patterns for multy containers:

    1) Colocated containers

       Just add 2 containers to the array, no guarantee that one starts before another


    2) Init container

apiVersion: v1
kind: Pod
metadata:
  name: pod-with-init-container
spec:
  initContainers:
    - name: init-download-data
      image: busybox
      command: ["sh", "-c", "echo Initializing... && sleep 5"]
  containers:
    - name: main-app
      image: nginx
      ports:
        - containerPort: 80


    3) Sidecar container

apiVersion: v1
kind: Pod
metadata:
  name: init-container-restart-always
spec:
  initContainers:
    - name: init-wait-for-service
      image: busybox
      restartPolicy: Always

  containers:
    - name: main-app
      image: nginx
      ports:
        - containerPort: 80


Side car will start first, the main app will wait till it started and starts only after that.
Side car will be stopped after the main container stopped. Good example - logs app - needs to be intitiated befor main app.

                                        Difference between controller and replicaSet

Deployment is a higher-level controller that manages ReplicaSets and provides rolling updates, rollbacks, and versioning.
ReplicaSet is a low-level controller that only ensures a fixed number of Pods are running.

üëâ You usually create Deployments, not ReplicaSets.


                            *** NOTE ***

If you try to drain a node and it has a node not mananged by replicaSet or deployment - you will get an error
because you it will not be replicated on other nodes


                            SECURITY (Most complex, needs more focus) –ü—Ä–æ–≥–Ω–∞—Ç—å –≤—Å—é —Å–µ–∫—Ü–∏—é 2 —Ä–∞–∑–∞ !!!

You cannot create users directly in kubernates, or view a list of users, but you can create service accounts:

    kubectl create serviceaccount sa1


PKI - public key infrastructure - the whole layer of all the certificates and keys

    Naming convention:

Certificate, public key  named :       *.crt       or       *,.pem

Private key ;                          *.key       or       *-.key.pem






